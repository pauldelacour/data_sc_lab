{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from yellowbrick.regressor import AlphaSelection\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, DotProduct, WhiteKernel, RBF, RationalQuadratic, Matern, ExpSineSquared\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_with_f_regressor_and_random_forest(n_features_fr, n_features_rf, X_train, X_val, y_train):\n",
    "    \n",
    "    # Using f_regression\n",
    "    features_scores = f_regression(X_train, y_train)[0]\n",
    "    y = list(features_scores)\n",
    "    myarray = np.asarray(y)\n",
    "\n",
    "    indices_fr = myarray.argsort()[-n_features_fr:][::-1]\n",
    "    \n",
    "    # Using Random Forest Regressor\n",
    "    rf = RandomForestRegressor(n_jobs=-1, n_estimators=50)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    scores = list(rf.feature_importances_)\n",
    "    my_rf_features = np.asarray(scores)\n",
    "\n",
    "    indices_rf = my_rf_features.argsort()[-n_features_rf:][::-1]\n",
    "\n",
    "    # Make the union of the two\n",
    "    indices = list(np.union1d(indices_rf, indices_fr))\n",
    "    \n",
    "    return X_train.iloc[:, indices], X_val.iloc[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers_samples_isolation_forest(X_train, y_train):\n",
    "    \n",
    "    # Use isolation forest for outlier detection, Computation heavy\n",
    "    forest = IsolationForest()\n",
    "    forest.fit(X_train)\n",
    "    \n",
    "    # Outlier indices for training\n",
    "    outliers_training = forest.predict(X_train)\n",
    "    outliers_training_indices = np.argwhere(outliers_training == -1).flatten()\n",
    "    \n",
    "    # Drop signal outliers in training data\n",
    "    X_train_without_outliers = X_train.drop(index=outliers_training_indices)\n",
    "    y_train_without_outliers = y_train.drop(index=outliers_training_indices)\n",
    "\n",
    "    return X_train_without_outliers, y_train_without_outliers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_lasso(X_train , X_test, y_train):\n",
    "    alphas = np.logspace(-1, 10, 100)\n",
    "    reg = LassoCV(cv = 5, alphas= alphas)\n",
    "    lasso = AlphaSelection(reg)\n",
    "    lasso.fit(X_train , y_train)\n",
    "    alpha_best = lasso.alpha_\n",
    "    coef = lasso.coef_\n",
    "    print(\"Best Alpha = \" , alpha_best)\n",
    "    print('# of coef before = ' , len(coef))\n",
    "    print(\"# of coef after = \" , np.sum(coef!=0))\n",
    "    X_train_extracted = X_train.loc[:,coef!=0]\n",
    "    X_test_extracted = X_test.loc[:,coef!=0]\n",
    "    return (X_train_extracted , X_test_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataframe, test_train_ratio):\n",
    "\n",
    "    total_samples = len(dataframe.index)\n",
    "    nsamples_test = int(test_train_ratio*total_samples)\n",
    "    dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    test = dataframe.iloc[:nsamples_test]\n",
    "    X_test = test.drop(['Age'], axis=1)\n",
    "    y_test = test[['Patient_ID', 'Age']]\n",
    "\n",
    "    train = dataframe.iloc[nsamples_test:]\n",
    "    X_train = train.drop(['Age'], axis=1)\n",
    "    y_train = train[['Patient_ID','Age']]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_median(dataframe_with_nan):\n",
    "    \n",
    "    for column in dataframe_with_nan:\n",
    "        if not isinstance(dataframe_with_nan[column].values[0], str): \n",
    "            median = dataframe_with_nan[column].median()\n",
    "            dataframe_with_nan[column].fillna(median, inplace=True)\n",
    "\n",
    "    return dataframe_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_by_correlation(X_train, y_train, nb_features):\n",
    "    corr = X_train.corrwith(y_train['Age'], axis=0, method='pearson')\n",
    "    feature_select = pd.DataFrame(corr.iloc[(-corr.abs()).argsort()][:nb_features])\n",
    "    best_feature_names = feature_select.index.values\n",
    "    \n",
    "    return feature_select, best_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_by_correlation(X_train, y_train, X_test, nb_features):\n",
    "    feature_select, best_feature_names = feature_select_by_correlation(X_train, y_train, nb_features)\n",
    "    best_feature_names = np.insert(best_feature_names, 0, 'Patient_ID')\n",
    "    return X_train[best_feature_names], X_test[best_feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_neural_network(dropout, X_train, y_train):\n",
    "\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=len(indices), kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "\n",
    "    model.add(Dense(1, init='RandomUniform'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[coeff_determination])\n",
    "    \n",
    "    # Fit the model\n",
    "    print(\"Start fitting ...\")\n",
    "    model.fit(x=X_train, y=y_train['y'], epochs=80, verbose=0, validation_split=0.1, shuffle=True, \\\n",
    "              steps_per_epoch=50, initial_epoch=0, validation_steps=5)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_adaboost_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create random forest object\n",
    "    ada = AdaBoostRegressor()\n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'base_estimator':[\n",
    "                    DecisionTreeRegressor(max_depth=25),\n",
    "                    DecisionTreeRegressor(max_depth=30),\n",
    "                    DecisionTreeRegressor(max_depth=35),\n",
    "                    DecisionTreeRegressor(max_depth=40)\n",
    "    ]\n",
    "                  ,'n_estimators':[1000], 'loss':['square']}\n",
    "    clf = GridSearchCV(ada, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters for AdaBoost: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_simple_linear_regression(X_train, y_train):\n",
    "    \n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train['y'])\n",
    "    \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgboost_with_grid_search(X_train, y_train):\n",
    "\n",
    "    # Create gradient boosting object\n",
    "    xgbr = XGBRegressor(verbosity=1, max_depth=10, reg_lambda=1) \n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'max_depth':[1,2,4], 'reg_lambda':[4, 6, 8, 10, 12], 'min_child_weight':[6, 8, 12,16]}    \n",
    "    clf = GridSearchCV(xgbr, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train['y'])\n",
    "    \n",
    "    print(\"Best parameters for XGBoost: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svr_with_grid_search(X_train, y_train):\n",
    "\n",
    "    # Create svr object\n",
    "    svr = SVR()\n",
    "    # svr = SVR(kernel='rbf', C=30, gamma=1)\n",
    "    # svr = SVR(kernel='linear', C=100, gamma='auto')\n",
    "    # svr = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1)\n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'kernel':['poly', 'rbf', 'sigmoid',],'C':[10e5, 10e6, 10e7], 'gamma':[10e-2, 10e-1, 1, 2]}\n",
    "    clf = GridSearchCV(svr, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters for SVR: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create random forest object\n",
    "    rf = RandomForestRegressor() \n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'max_depth':[10, 15, 20, 25, 30], \n",
    "                  'n_estimators' :[1000],\n",
    "                  'min_samples_split':[2, 3, 5],\n",
    "                  'max_leaf_nodes':[100, 150, 200, 250]}\n",
    "    clf = GridSearchCV(rf, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters for Random Forest: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConstantKernel, DotProduct, WhiteKernel, RBF, RationalQuadratic, Matern, ExpSineSquared\n",
    "def fit_gaussian_process_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create Gaussian process object\n",
    "    gpr = GaussianProcessRegressor()\n",
    "    \n",
    "    # Grid search\n",
    "    parameters = {\n",
    "        'kernel': [10 * RationalQuadratic() * DotProduct(sigma_0 = 1) + 2 * ConstantKernel() + WhiteKernel(noise_level=0.5)\n",
    "                  ]}\n",
    "    clf = GridSearchCV(gpr, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters for Gaussian Process: \" + str(clf.best_params_))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gradient_boosting_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create Gradient boosting object\n",
    "    gb = GradientBoostingRegressor()\n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'learning_rate':[0.06, 0.05, 0.04], \n",
    "                         'n_estimators':[1000],\n",
    "                         'min_samples_split':[2], \n",
    "                         'max_depth':[2, 4, 6]}    \n",
    "    clf = GridSearchCV(gb, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters for Gradient Boosting: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_extra_trees_regressor_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create Gradient boosting object\n",
    "    etr = ExtraTreesRegressor(n_jobs=1)\n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'max_depth':[2, 3, 5, 10, 15], \n",
    "                         'n_estimators':[1000],\n",
    "                         'min_samples_split':[2]\n",
    "                 }        \n",
    "    clf = GridSearchCV(etr, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters for Extra Trees Regressor: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_simple_linear_regression(X_train_feature_extracted, y_train, nb_cv):\n",
    "\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train_feature_extracted, y_train)\n",
    "\n",
    "    # Evaluate performance using cross-validation \n",
    "    scores = cross_val_score(regr, X_train_feature_extracted, y_train, scoring=\"neg_mean_squared_error\", cv=nb_cv)\n",
    "    train_mse = np.mean(-scores)\n",
    "\n",
    "    # Results\n",
    "    print(\"----- Train results -------\")\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    print('Mean squared error (cross-validation): %.2f'\n",
    "          % train_mse)\n",
    "    print()\n",
    "\n",
    "    return regr, train_mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_regression(X_train_feature_extracted, X_test_feature_extracted, y_train, y_test, nb_cv):\n",
    "\n",
    "    regr, train_mse = fit_simple_linear_regression(X_train_feature_extracted, y_train, nb_cv)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_test_feature_extracted)\n",
    "    test_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(\"----- Test results --------\")\n",
    "    print(\"Number of test samples: \", len(y_pred))\n",
    "    print('Mean squared error: %.2f'\n",
    "          % test_mse)\n",
    "    \n",
    "    return regr, train_mse, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slr_with_correlation(aggregated_dataframe, test_ratio, nb_features_extracted, nb_cv):\n",
    "    \n",
    "    # Define Train and Test data \n",
    "    print()\n",
    "    print(\"---- Split into Train and Test data ----\")\n",
    "    print()\n",
    "    (X_train_inter, y_train_inter, X_test_inter, y_test_inter) = \\\n",
    "                                                    train_test_split(aggregated_dataframe, test_ratio)\n",
    "\n",
    "    print(\"Length of the train set:\")\n",
    "    print(len(y_train_inter))\n",
    "    print(\"Length of the test set:\")\n",
    "    print(len(y_test_inter))\n",
    "    \n",
    "    # Feature selection by correlation\n",
    "    print()\n",
    "    print(\"---- Feature selection by correlation ----\")\n",
    "    print()\n",
    "    feature_select, best_feature_names = \\\n",
    "                        feature_select_by_correlation(X_train_inter, y_train_inter, nb_features_extracted)\n",
    "    print(feature_select)\n",
    "    \n",
    "    X_train_inter_feature_extracted, X_test_inter_feature_extracted = \\\n",
    "               feature_selection_by_correlation(X_train_inter, y_train_inter, X_test_inter, nb_features_extracted)\n",
    "    \n",
    "    # Simple Linear Regression\n",
    "    print()\n",
    "    print(\"---- Simple Linear Regression ----\")\n",
    "    print()\n",
    "    \n",
    "    # Remove key for linear regression\n",
    "    X_train_inter_feature_extracted = X_train_inter_feature_extracted.drop(['Patient_ID'], axis=1, errors='ignore')\n",
    "    X_test_inter_feature_extracted = X_test_inter_feature_extracted.drop(['Patient_ID'], axis=1, errors='ignore')\n",
    "\n",
    "    y_train_inter = y_train_inter.drop(['Patient_ID'], axis=1, errors='ignore')\n",
    "    y_test_inter = y_test_inter.drop(['Patient_ID'], axis=1, errors='ignore')\n",
    "    \n",
    "    simple_linear_regression(X_train_inter_feature_extracted, X_test_inter_feature_extracted, \\\n",
    "                         y_train_inter, y_test_inter, nb_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_slr_with_correlation(df_label_psd_cluster, df_label_spectro_cluster, df_label_microstate, \n",
    "                                 test_ratio, nb_features_extracted_psd, nb_features_extracted_spe, \n",
    "                                nb_features_extracted_mic, nb_cv):\n",
    "    \n",
    "    # Define Train and Test data \n",
    "    print()\n",
    "    print(\"---- Split into Train and Test data ----\")\n",
    "    print()\n",
    "    (X_train_psd, y_train_psd, X_test_psd, y_test_psd) = \\\n",
    "                                                        train_test_split(df_label_psd_cluster, test_ratio)\n",
    "    (X_train_spe, y_train_spe, X_test_spe, y_test_spe) = \\\n",
    "                                                        train_test_split(df_label_spectro_cluster, test_ratio)\n",
    "    (X_train_mic, y_train_mic, X_test_mic, y_test_mic) = \\\n",
    "                                                        train_test_split(df_label_microstate, test_ratio)\n",
    "\n",
    "    print(\"Test lengths: \" + str(len(y_test_psd)) + \"(psd), \" + \\\n",
    "         str(len(y_test_spe)) + \"(spectro), \" + str(len(y_test_mic)) + \"(micro)\")\n",
    "    \n",
    "    # Feature selection by correlation\n",
    "    print()\n",
    "    print(\"---- Feature selection by correlation ----\")\n",
    "    print()\n",
    "    X_train_psd_feature_extracted, X_test_psd_feature_extracted = \\\n",
    "            feature_selection_by_correlation(X_train_psd, y_train_psd, X_test_psd, nb_features_extracted_psd)\n",
    "    X_train_spe_feature_extracted, X_test_spe_feature_extracted = \\\n",
    "            feature_selection_by_correlation(X_train_spe, y_train_spe, X_test_spe, nb_features_extracted_spe)\n",
    "    X_train_mic_feature_extracted, X_test_mic_feature_extracted = \\\n",
    "            feature_selection_by_correlation(X_train_mic, y_train_mic, X_test_mic, nb_features_extracted_mic)\n",
    "    \n",
    "    # Simple Linear Regressions\n",
    "    print()\n",
    "    print(\"---- Simple Linear Regressions ----\")\n",
    "    print()\n",
    "\n",
    "    # Train Power Spectrum Cluster SLR\n",
    "    regr_psd, train_mse_psd = fit_simple_linear_regression(\n",
    "        X_train_psd_feature_extracted.drop(['Patient_ID'], axis=1, errors='ignore'),\n",
    "        y_train_psd.drop(['Patient_ID'], axis=1, errors='ignore'), \n",
    "        nb_cv)\n",
    "\n",
    "    # Train Spectro SLR\n",
    "    regr_spe, train_mse_spe = fit_simple_linear_regression(\n",
    "        X_train_spe_feature_extracted.drop(['Patient_ID'], axis=1, errors='ignore'),\n",
    "        y_train_spe.drop(['Patient_ID'], axis=1, errors='ignore'), \n",
    "        nb_cv)\n",
    "\n",
    "    # Train Microstate\n",
    "    regr_mic, train_mse_mic = fit_simple_linear_regression(\n",
    "        X_train_mic_feature_extracted.drop(['Patient_ID'], axis=1, errors='ignore'),\n",
    "        y_train_mic.drop(['Patient_ID'], axis=1, errors='ignore'),\n",
    "        nb_cv)\n",
    "    \n",
    "    # Make the union of the test data sets\n",
    "    union_test_patient_ids = pd.merge(X_test_psd['Patient_ID'], X_test_spe['Patient_ID'], \\\n",
    "                                      on='Patient_ID', how='outer')\n",
    "    union_test_patient_ids = pd.merge(union_test_patient_ids, X_test_mic['Patient_ID'], \\\n",
    "                                      on='Patient_ID', how='outer')\n",
    "\n",
    "    # Make an ensemble testing, weightened by train mse score\n",
    "\n",
    "    sse = 0\n",
    "    for test_patient_id in union_test_patient_ids[\"Patient_ID\"]:\n",
    "\n",
    "        y_pred = [0, 0, 0]\n",
    "        weights = [0, 0, 0]\n",
    "\n",
    "        if test_patient_id in X_test_psd['Patient_ID'].values:\n",
    "            test_sample = X_test_psd_feature_extracted[X_test_psd_feature_extracted['Patient_ID']==test_patient_id]\n",
    "            test_sample = test_sample.drop(['Patient_ID'], axis=1)\n",
    "\n",
    "            y_pred[0] = regr_psd.predict(test_sample)[0][0]\n",
    "            weights[0] = 1.0/train_mse_psd\n",
    "            y_test = y_test_psd[y_test_psd['Patient_ID']==test_patient_id]['Age'].values[0]\n",
    "\n",
    "        if test_patient_id in X_test_spe['Patient_ID'].values:\n",
    "            test_sample = X_test_spe_feature_extracted[X_test_spe_feature_extracted['Patient_ID']==test_patient_id]\n",
    "            test_sample = test_sample.drop(['Patient_ID'], axis=1)\n",
    "\n",
    "            y_pred[1] = regr_spe.predict(test_sample)[0][0]\n",
    "            weights[1] = 1.0/train_mse_spe\n",
    "            y_test = y_test_spe[y_test_spe['Patient_ID']==test_patient_id]['Age'].values[0]\n",
    "\n",
    "        if test_patient_id in X_test_mic['Patient_ID'].values:\n",
    "            test_sample = X_test_mic_feature_extracted[X_test_mic_feature_extracted['Patient_ID']==test_patient_id]\n",
    "            test_sample = test_sample.drop(['Patient_ID'], axis=1)\n",
    "\n",
    "            y_pred[2] = regr_mic.predict(test_sample)[0][0]\n",
    "            weights[2] = 1.0/train_mse_mic\n",
    "            y_test = y_test_mic[y_test_mic['Patient_ID']==test_patient_id]['Age'].values[0]\n",
    "\n",
    "        weights = weights/sum(weights)\n",
    "        y_pred = np.sum(np.multiply(y_pred, weights))\n",
    "\n",
    "        sse = sse + (y_pred - y_test)**2\n",
    "\n",
    "    test_mse = sse/len(union_test_patient_ids)\n",
    "    print(\"----- Test results --------\")\n",
    "    print('Mean squared error: %.2f'% test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gradient_boosting(X_train, y_train, nb_cv=10):\n",
    "\n",
    "    # Create gradient boosting object\n",
    "    # verbosity=1, max_depth=15, min_child_weight=100, subsample=1, reg_lambda=3\n",
    "    xgbr = XGBRegressor(verbosity=1, max_depth=15, min_child_weight=100, subsample=1, reg_lambda=4) \n",
    "   \n",
    "    # Train the model using the training sets\n",
    "    xgbr.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate performance using cross-validation \n",
    "    scores = cross_val_score(xgbr, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=nb_cv)\n",
    "    train_mse = np.mean(-scores)\n",
    "\n",
    "    # Results\n",
    "    print(\"----- Train results -------\")\n",
    "    print('Mean squared error (cross-validation): %.2f'\n",
    "          % train_mse)\n",
    "    print()\n",
    "    \n",
    "    return xgbr, train_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_xgboost_with_correlation(df_label_psd_cluster, df_label_spectro_cluster, df_label_microstate, \n",
    "                                 test_ratio, nb_features_extracted_psd, nb_features_extracted_spe, \n",
    "                                nb_features_extracted_mic, nb_cv):\n",
    "    \n",
    "    # Define Train and Test data \n",
    "    print()\n",
    "    print(\"---- Split into Train and Test data ----\")\n",
    "    print()\n",
    "    (X_train_psd, y_train_psd, X_test_psd, y_test_psd) = \\\n",
    "                                                        train_test_split(df_label_psd_cluster, test_ratio)\n",
    "    (X_train_spe, y_train_spe, X_test_spe, y_test_spe) = \\\n",
    "                                                        train_test_split(df_label_spectro_cluster, test_ratio)\n",
    "    (X_train_mic, y_train_mic, X_test_mic, y_test_mic) = \\\n",
    "                                                        train_test_split(df_label_microstate, test_ratio)\n",
    "\n",
    "    print(\"Test lengths: \" + str(len(y_test_psd)) + \"(psd), \" + \\\n",
    "         str(len(y_test_spe)) + \"(spectro), \" + str(len(y_test_mic)) + \"(micro)\")\n",
    "    \n",
    "    # Feature selection by correlation\n",
    "    print()\n",
    "    print(\"---- Feature selection by correlation ----\")\n",
    "    print()\n",
    "    X_train_psd_feature_extracted, X_test_psd_feature_extracted = X_train_psd, X_test_psd\n",
    "            #feature_selection_by_correlation(X_train_psd, y_train_psd, X_test_psd, nb_features_extracted_psd)\n",
    "    X_train_spe_feature_extracted, X_test_spe_feature_extracted = X_train_spe, X_test_spe\n",
    "            #feature_selection_by_correlation(X_train_spe, y_train_spe, X_test_spe, nb_features_extracted_spe)\n",
    "    X_train_mic_feature_extracted, X_test_mic_feature_extracted = X_train_mic, X_test_mic\n",
    "            #feature_selection_by_correlation(X_train_mic, y_train_mic, X_test_mic, nb_features_extracted_mic)\n",
    "    \n",
    "    # XGBoost Regressions\n",
    "    print()\n",
    "    print(\"---- XGBoost Regressions ----\")\n",
    "    print()\n",
    "\n",
    "    # Train Power Spectrum Cluster SLR\n",
    "    regr_psd, train_mse_psd = fit_gradient_boosting(\n",
    "        X_train_psd_feature_extracted.drop(['Patient_ID'], axis=1, errors='ignore'),\n",
    "        y_train_psd.drop(['Patient_ID'], axis=1, errors='ignore'), \n",
    "        nb_cv)\n",
    "\n",
    "    # Train Spectro SLR\n",
    "    regr_spe, train_mse_spe = fit_gradient_boosting(\n",
    "        X_train_spe_feature_extracted.drop(['Patient_ID'], axis=1, errors='ignore'),\n",
    "        y_train_spe.drop(['Patient_ID'], axis=1, errors='ignore'), \n",
    "        nb_cv)\n",
    "\n",
    "    # Train Microstate\n",
    "    regr_mic, train_mse_mic = fit_gradient_boosting(\n",
    "        X_train_mic_feature_extracted.drop(['Patient_ID'], axis=1, errors='ignore'),\n",
    "        y_train_mic.drop(['Patient_ID'], axis=1, errors='ignore'),\n",
    "        nb_cv)\n",
    "    \n",
    "    # Make the union of the test data sets\n",
    "    union_test_patient_ids = pd.merge(X_test_psd['Patient_ID'], X_test_spe['Patient_ID'], \\\n",
    "                                      on='Patient_ID', how='outer')\n",
    "    union_test_patient_ids = pd.merge(union_test_patient_ids, X_test_mic['Patient_ID'], \\\n",
    "                                      on='Patient_ID', how='outer')\n",
    "\n",
    "    # Make an ensemble testing, weightened by train mse score\n",
    "\n",
    "    sse = 0\n",
    "    for test_patient_id in union_test_patient_ids[\"Patient_ID\"]:\n",
    "\n",
    "        y_pred = [0, 0, 0]\n",
    "        weights = [0, 0, 0]\n",
    "\n",
    "        if test_patient_id in X_test_psd['Patient_ID'].values:\n",
    "            test_sample = X_test_psd_feature_extracted[X_test_psd_feature_extracted['Patient_ID']==test_patient_id]\n",
    "            test_sample = test_sample.drop(['Patient_ID'], axis=1)\n",
    "            \n",
    "            y_pred[0] = regr_psd.predict(test_sample)[0]\n",
    "            weights[0] = 1.0/train_mse_psd\n",
    "            y_test = y_test_psd[y_test_psd['Patient_ID']==test_patient_id]['Age'].values[0]\n",
    "\n",
    "        if test_patient_id in X_test_spe['Patient_ID'].values:\n",
    "            test_sample = X_test_spe_feature_extracted[X_test_spe_feature_extracted['Patient_ID']==test_patient_id]\n",
    "            test_sample = test_sample.drop(['Patient_ID'], axis=1)\n",
    "\n",
    "            y_pred[1] = regr_spe.predict(test_sample)[0]\n",
    "            weights[1] = 1.0/train_mse_spe\n",
    "            y_test = y_test_spe[y_test_spe['Patient_ID']==test_patient_id]['Age'].values[0]\n",
    "\n",
    "        if test_patient_id in X_test_mic['Patient_ID'].values:\n",
    "            test_sample = X_test_mic_feature_extracted[X_test_mic_feature_extracted['Patient_ID']==test_patient_id]\n",
    "            test_sample = test_sample.drop(['Patient_ID'], axis=1)\n",
    "\n",
    "            y_pred[2] = regr_mic.predict(test_sample)[0]\n",
    "            weights[2] = 1.0/train_mse_mic\n",
    "            y_test = y_test_mic[y_test_mic['Patient_ID']==test_patient_id]['Age'].values[0]\n",
    "\n",
    "        weights = weights/sum(weights)\n",
    "        y_pred = np.sum(np.multiply(y_pred, weights))\n",
    "\n",
    "        sse = sse + (y_pred - y_test)**2\n",
    "\n",
    "    test_mse = sse/len(union_test_patient_ids)\n",
    "    print(\"----- Test results --------\")\n",
    "    print('Mean squared error: %.2f'% test_mse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgboost_with_grid_search(X_train, y_train):\n",
    "\n",
    "    # Create gradient boosting object\n",
    "    xgbr = XGBRegressor(verbosity=1) \n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'max_depth':[1,2,4], 'reg_lambda':[8, 10, 12], 'min_child_weight':[6, 8, 12,16]}\n",
    "    clf = GridSearchCV(xgbr, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters for XGBoost: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
