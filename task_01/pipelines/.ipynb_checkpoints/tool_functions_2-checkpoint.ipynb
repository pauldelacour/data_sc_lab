{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from yellowbrick.regressor import AlphaSelection\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, DotProduct, WhiteKernel, RBF, RationalQuadratic, Matern, ExpSineSquared\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_neural_network(dropout, X_train, y_train):\n",
    "\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=len(indices), kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "\n",
    "    model.add(Dense(1, init='RandomUniform'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[coeff_determination])\n",
    "    \n",
    "    # Fit the model\n",
    "    print(\"Start fitting ...\")\n",
    "    model.fit(x=X_train, y=y_train['y'], epochs=80, verbose=0, validation_split=0.1, shuffle=True, \\\n",
    "              steps_per_epoch=50, initial_epoch=0, validation_steps=5)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_extra_trees_regressor_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create Gradient boosting object\n",
    "    etr = ExtraTreesRegressor(n_jobs=1)\n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'max_depth':[10, 15, 20, 25], \n",
    "                         'n_estimators':[1000],\n",
    "                         'min_samples_split':[2]\n",
    "                 }    \n",
    "    clf = GridSearchCV(etr, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train['y'])\n",
    "    \n",
    "    print(\"Best parameters for Extra Trees Regressor: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gradient_boosting_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create Gradient boosting object\n",
    "    gb = GradientBoostingRegressor()\n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'learning_rate':[0.06, 0.05, 0.04], \n",
    "                         'n_estimators':[1000],\n",
    "                         'min_samples_split':[2], \n",
    "                         'max_depth':[2, 4, 6]}    \n",
    "    clf = GridSearchCV(gb, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train['y'])\n",
    "    \n",
    "    print(\"Best parameters for Gradient Boosting: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConstantKernel, DotProduct, WhiteKernel, RBF, RationalQuadratic, Matern, ExpSineSquared\n",
    "def fit_gaussian_process_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create Gaussian process object\n",
    "    gpr = GaussianProcessRegressor()\n",
    "    \n",
    "    # Grid search\n",
    "    parameters = {\n",
    "        'kernel': [10 * RationalQuadratic() * DotProduct(sigma_0 = 1) + 2 * ConstantKernel() + WhiteKernel(noise_level=0.5)\n",
    "                  ]}\n",
    "    clf = GridSearchCV(gpr, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train['y'])\n",
    "    \n",
    "    print(\"Best parameters for Gaussian Process: \" + str(clf.best_params_))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_adaboost_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create random forest object\n",
    "    ada = AdaBoostRegressor()\n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'base_estimator':[\n",
    "                    DecisionTreeRegressor(max_depth=35),\n",
    "                    DecisionTreeRegressor(max_depth=40),\n",
    "                    DecisionTreeRegressor(max_depth=45)\n",
    "    ]\n",
    "                  ,'n_estimators':[1000], 'loss':['square']}\n",
    "    clf = GridSearchCV(ada, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train['y'])\n",
    "    \n",
    "    print(\"Best parameters for AdaBoost: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest_with_grid_search(X_train, y_train):\n",
    "    \n",
    "    # Create random forest object\n",
    "    rf = RandomForestRegressor() \n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'max_depth':[10, 15, 20], \n",
    "                  'n_estimators' :[1000],\n",
    "                  'min_samples_split':[2, 3, 5],\n",
    "                  'max_leaf_nodes':[100, 150]}\n",
    "    clf = GridSearchCV(rf, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train['y'])\n",
    "    \n",
    "    print(\"Best parameters for Random Forest: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_simple_linear_regression(X_train, y_train):\n",
    "    \n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train['y'])\n",
    "    \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgboost_with_grid_search(X_train, y_train):\n",
    "\n",
    "    # Create gradient boosting object\n",
    "    xgbr = XGBRegressor(verbosity=1, max_depth=10, reg_lambda=1) \n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'max_depth':[1,2,4], 'reg_lambda':[8, 10, 12], 'min_child_weight':[6, 8, 12,16]}\n",
    "    clf = GridSearchCV(xgbr, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train['y'])\n",
    "    \n",
    "    print(\"Best parameters for XGBoost: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svr_with_grid_search(X_train, y_train):\n",
    "\n",
    "    # Create svr object\n",
    "    svr = SVR()\n",
    "    # svr = SVR(kernel='rbf', C=30, gamma=1)\n",
    "    # svr = SVR(kernel='linear', C=100, gamma='auto')\n",
    "    # svr = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1)\n",
    "    \n",
    "    # Grid search \n",
    "    parameters = {'kernel':['rbf'],'C':[10e5, 10e6, 10e7, 10e8], 'gamma':[10e-2, 10e-1, 1, 2, 4, 8]}\n",
    "    clf = GridSearchCV(svr, parameters, scoring='r2', n_jobs=4, iid=False, cv=5)\n",
    "    clf.fit(X_train, y_train['y'])\n",
    "    \n",
    "    print(\"Best parameters for SVR: \" + str(clf.best_params_))\n",
    "            \n",
    "    return clf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_lasso(X_train , X_val, y_train):\n",
    "    alphas = np.logspace(-1, 10, 100)\n",
    "    reg = LassoCV(cv = 5, alphas= alphas)\n",
    "    lasso = AlphaSelection(reg)\n",
    "    lasso.fit(X_train , y_train['y'])\n",
    "    alpha_best = lasso.alpha_\n",
    "    coef = lasso.coef_\n",
    "    print(\"Best Alpha = \" , alpha_best)\n",
    "    print('# of coef before = ' , len(coef))\n",
    "    print(\"# of coef after = \" , np.sum(coef!=0))\n",
    "    X_train_extracted = X_train.loc[:,coef!=0]\n",
    "    X_val_extracted = X_val.loc[:,coef!=0]\n",
    "    return (X_train_extracted , X_val_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_by_correlation(X_train, y_train, X_val, nb_features):\n",
    "    corr = X_train.corrwith(y_train['y'], axis=0, method='pearson')\n",
    "    \n",
    "    feature_select = pd.DataFrame(corr.iloc[(-corr.abs()).argsort()][:nb_features])\n",
    "    best_feature_names = feature_select.index.values\n",
    "    \n",
    "    # Feature selection of the training set\n",
    "    X_train_feature_extracted = X_train[best_feature_names]\n",
    "\n",
    "    # Feature selection of the testing set\n",
    "    X_val_feature_extracted = X_val[best_feature_names]\n",
    "    \n",
    "    return X_train_feature_extracted, X_val_feature_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers_samples_gaussian_selection(X_train, y_train):\n",
    "    \n",
    "    # Use Elliptic Enveloppe for outlier detection, Computation heavy\n",
    "    elenv = EllipticEnvelope(support_fraction=0.95)\n",
    "    elenv.fit(X_train)\n",
    "    \n",
    "    # Outlier indices for training\n",
    "    outliers_training = elenv.predict(X_train)\n",
    "    outliers_training_indices = np.argwhere(outliers_training == -1).flatten()\n",
    "    \n",
    "    # Drop signal outliers in training data\n",
    "    X_train_without_outliers = X_train.drop(index=outliers_training_indices)\n",
    "    y_train_without_outliers = y_train.drop(index=outliers_training_indices)\n",
    "\n",
    "    return X_train_without_outliers, y_train_without_outliers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers_samples_isolation_forest(X_train, y_train):\n",
    "    \n",
    "    # Use isolation forest for outlier detection, Computation heavy\n",
    "    forest = IsolationForest()\n",
    "    forest.fit(X_train)\n",
    "    \n",
    "    # Outlier indices for training\n",
    "    outliers_training = forest.predict(X_train)\n",
    "    outliers_training_indices = np.argwhere(outliers_training == -1).flatten()\n",
    "    \n",
    "    # Drop signal outliers in training data\n",
    "    X_train_without_outliers = X_train.drop(index=outliers_training_indices)\n",
    "    y_train_without_outliers = y_train.drop(index=outliers_training_indices)\n",
    "\n",
    "    return X_train_without_outliers, y_train_without_outliers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_with_median(dataframe_with_nan, feature_medians_training):\n",
    "    column_index = 0\n",
    "    for column in dataframe_with_nan.columns:\n",
    "        dataframe_with_nan[column].fillna(feature_medians_training[column_index], inplace=True)\n",
    "        column_index = column_index + 1\n",
    "\n",
    "    return dataframe_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_2_median(X_train, X_val):\n",
    "    \n",
    "    # Compute median of each feature\n",
    "    feature_medians_training = np.nanmedian(X_train, axis=0)\n",
    "    \n",
    "    # Transform training Data\n",
    "    X_train_with_median = fill_nan_with_median(X_train, feature_medians_training)\n",
    "\n",
    "    # Tranform validation Data using Training Data\n",
    "    X_val_with_median = fill_nan_with_median(X_val, feature_medians_training)\n",
    "    \n",
    "    return X_train_with_median, X_val_with_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_with_f_regressor_and_random_forest(n_features_fr, n_features_rf, X_train, X_val, y_train):\n",
    "    \n",
    "    # Using f_regression\n",
    "    features_scores = f_regression(X_train, y_train['y'])[0]\n",
    "    y = list(features_scores)\n",
    "    myarray = np.asarray(y)\n",
    "\n",
    "    indices_fr = myarray.argsort()[-n_features_fr:][::-1]\n",
    "    \n",
    "    # Using Random Forest Regressor\n",
    "    rf = RandomForestRegressor(n_jobs=-1, n_estimators=50)\n",
    "    rf.fit(X_train, y_train['y'])\n",
    "\n",
    "    scores = list(rf.feature_importances_)\n",
    "    my_rf_features = np.asarray(scores)\n",
    "\n",
    "    indices_rf = my_rf_features.argsort()[-n_features_rf:][::-1]\n",
    "\n",
    "    # Make the union of the two\n",
    "    indices = list(np.union1d(indices_rf, indices_fr))\n",
    "    \n",
    "    return X_train.iloc[:, indices], X_val.iloc[:, indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
