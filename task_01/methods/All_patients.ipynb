{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mathieuchevalley/Documents/Study/ETHZ/Fall Semester 2020/Data Science lab/data_sc_lab/task_01/pipelines\n"
     ]
    }
   ],
   "source": [
    "cd ../pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipynb.fs.full.tool_functions import *\n",
    "from yellowbrick.regressor import AlphaSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (66,68,80,81,82,83,84,95,96,97,98,99,110,111,112,113,114,125,126,127,128,129,140,141,142,144,155,156,157,159,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1384,1385,1387,1390,1391,1393,1396,1397,1399,1402,1403,1405,1408,1409,1411,1414,1415,1417,1420,1421,1423,1426,1427,1429,1432,1433,1435,1438,1439,1441,1444,1445,1447,1450,1451,1453,1456,1457,1459,1462,1463,1465,1468,1469,1471,1474,1475,1477,1480,1481,1483,1486,1487,1489,1492,1493,1495,1498,1499,1501,1504,1505,1507,1510,1511,1513,1516,1517,1519,1522,1523,1525,1528,1529,1531,1534,1535,1537,1540,1541,1543,1546,1547,1549,1552,1553,1555,1558,1559,1561,1564,1565,1567,1570,1571,1573,1576,1577,1579,1582,1583,1585,1588,1589,1591,1594,1595,1597,1600,1601,1603,1606,1607,1609,1612,1613,1615,1618,1619,1621,1624,1625,1627,1630,1631,1633,1636,1637,1639,1642,1643,1645,1648,1649,1651,1654,1655,1657,1660,1661,1663,1666,1667,1669,1672,1673,1675,1678,1679,1681,1684,1685,1687,1690,1691,1693,1696,1697,1699,1702,1703,1705,1708,1709,1711,1714,1715,1717,1720,1721,1723,1726,1727,1729,1732,1733,1735,1738,1739,1741,1744,1745,1747,1750,1751,1753,1756,1757,1759,1762,1763,1765,1768,1769,1771,1774,1775,1777,1780,1781,1783,1786,1787,1789,1792,1793,1795,1798,1799,1801,1804,1805,1807,1810,1811,1813,1816,1817,1819,1822,1823,1825,1828,1829,1831,1834,1835,1837,1840,1841,1843,1846,1847,1849,1852,1853,1855,1858,1859,1861,1882,1883,1884,1917,1918,1919,1952,1953,1954,1955,1987,1988,1989,1990,2022,2023,2024,2025,2026,2057,2058,2059,2060,2061,2092,2093,2094,2095,2096,2097,2127,2128,2129,2130,2131,2132,2162,2163,2164,2165,2166,2167,2168,2197,2198,2199,2200,2201,2202,2203,2232,2233,2234,2235,2236,2237,2238,2239,2267,2268,2269,2270,2271,2272,2273,2274,2302,2303,2304,2305,2306,2307,2308,2309,2337,2338,2339,2340,2341,2342,2343,2344,2477,2478,2479,2512,2513,2514,2547,2548,2549,2550,2582,2583,2584,2585,2617,2618,2619,2620,2621,2652,2653,2654,2655,2656,2687,2688,2689,2690,2691,2692,2722,2723,2724,2725,2726,2727,2757,2758,2759,2760,2761,2762,2763,2792,2793,2794,2795,2796,2797,2798,2827,2828,2829,2830,2831,2832,2833,2834,2862,2863,2864,2865,2866,2867,2868,2869,2897,2898,2899,2900,2901,2902,2903,2904,2932,2933,2934,2935,2936,2937,2938,2939,3819,3826,3866,3933,4037,4055,4059,4063,4067,4071,4075,4079,4084,4085,4086,4087,4088,4091,4092,4094,4095,4096,4608,4616,4648,4649,4664,4665,4667,4683,4690,4691,4692,4693,4739,4800,5002,5010,5018,5026,5042,5240,5282,5386,5433,5443,5463,5466,5467,5468,5470,5478,5479,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5495,5499,5503,5507,5511,5515,5517,5518,5519,5520,5521,5522,5523,5524,5532,5535,5536,5537,5539,5540,5548,5549,5551,5552,5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5565,5569,5573,5577,5581,5586,5587,5588,5589,5590,5591,5592,5593,5594,5601,5604,5605,5606,5608,5616,5617,5619,5620,5621,5622,5623,5624,5625,5626,5627,5628,5629,5630,5633,5636,5637,5638,5640,5648,5649,5651,5652,5653,5654,5655,5656,5657,5658,5659,5660,5661,5662,5665,5668,5669,5670,5672,5680,5681,5683,5684,5685,5686,5687,5688,5689,5690,5691,5692,5693,5694,5697,5700,5701,5702,5704,5712,5713,5715,5716,5717,5718,5719,5720,5721,5722,5723,5724,5725,5726,5729,5732,5733,5734,5736,5744,5745,5747,5748,5749,5750,5751,5752,5753,5754,5755,5756,5757,5758,5759,5760,5761,5762,5763,5764,5768,5772,5776,5780,5784,5788,5792,5796,5800,5804,5808,5812,5816,5820,5869,5877,5881,5885,5889,5893,5897,6538,6539,6540,6541,6542,6555,6556,6557,6558,6559,6572,6573,6574,6575,6576,6589,6590,6591,6592,6593,6606,6607,6608,6609,6610,6623,6624,6625,6626,6627,6636,6644,6650,6653,6682,6701,6738,6768) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_beha = pd.read_csv('../../../data/Behavioral/AllData.csv')#, nrows=1000)\n",
    "\n",
    "df_psd_cluster = pd.read_csv(\"../../../data/EEG/RestingEEG_PSD_Cluster.csv\") #, nrows=1000)\n",
    "df_psd_channel = pd.read_csv(\"../../../data/EEG/RestingEEG_PSD_Channel.csv\") #, nrows=5)\n",
    "df_spectro_cluster = pd.read_csv(\"../../../data/EEG/RestingEEG_Spectro_Cluster.csv\") #, nrows=1000)\n",
    "df_spectro_channel = pd.read_csv(\"../../../data/EEG/RestingEEG_Spectro_Channel.csv\") #, nrows=5)\n",
    "\n",
    "df_microstate = pd.read_csv(\"../../../data/EEG/RestingEEG_Microstates.csv\") #, nrows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = df_beha[[\"Patient_ID\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spectro_cluster.rename(columns={'id': 'Patient_ID'}, inplace=True)\n",
    "df_label_spectro_cluster = pd.merge(y_labels, df_spectro_cluster, on='Patient_ID')\n",
    "df_label_spectro_cluster = fill_with_median(df_label_spectro_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_18 = df_label_spectro_cluster['Age'] <= 18.0\n",
    "df_label_spectro_cluster = df_label_spectro_cluster[less_18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Split into Train and Test data ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"---- Split into Train and Test data ----\")\n",
    "print()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test, y_train , y_test = \\\n",
    "train_test_split(df_label_spectro_cluster.drop(['Age', 'Patient_ID'], axis=1), df_label_spectro_cluster[['Age']], test_size=0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/iforest.py:237: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/iforest.py:247: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2    3    4    5    6    7    8    9   11   12   13   14\n",
      "   15   16   17   18   19   20   21   22   23   25   26   27   28   29\n",
      "   30   31   32   33   34   35   36   37   38   39   41   42   43   44\n",
      "   45   46   47   48   49   50   51   52   53   54   55   56   57   58\n",
      "   59   60   61   62   63   64   65   66   67   68   69   71   72   73\n",
      "   74   76   77   78   79   80   82   83   84   85   86   87   88   89\n",
      "   90   92   93   94   95   96   97   98   99  100  101  103  104  105\n",
      "  106  107  108  109  111  113  114  115  116  117  118  119  120  121\n",
      "  122  123  125  127  128  129  130  131  132  133  134  135  136  137\n",
      "  138  139  141  142  143  144  145  147  148  149  150  151  152  153\n",
      "  154  155  156  157  158  159  161  162  163  164  165  166  167  168\n",
      "  169  170  171  172  173  175  176  177  178  179  180  183  184  186\n",
      "  187  188  189  190  191  194  195  196  197  198  199  200  201  202\n",
      "  203  204  207  208  209  210  211  212  213  214  215  216  217  218\n",
      "  220  221  222  223  224  225  226  227  228  229  232  233  234  235\n",
      "  236  237  238  239  241  243  245  246  247  249  250  251  252  253\n",
      "  254  255  256  257  258  259  260  261  262  263  264  265  266  268\n",
      "  269  270  271  272  273  274  275  276  277  278  279  280  281  282\n",
      "  283  284  285  286  287  288  290  291  292  294  295  296  297  298\n",
      "  299  300  301  302  303  304  305  306  307  309  310  311  312  313\n",
      "  314  315  316  317  318  319  320  322  323  324  325  326  327  328\n",
      "  330  331  332  333  334  335  336  337  338  339  340  341  342  343\n",
      "  344  345  346  347  348  349  350  352  353  354  355  356  358  359\n",
      "  361  362  364  365  366  367  368  369  370  371  372  373  374  375\n",
      "  376  377  378  379  380  381  382  383  384  386  387  388  389  390\n",
      "  391  392  393  394  395  397  398  399  400  401  402  403  404  405\n",
      "  406  407  409  410  411  412  413  414  415  416  417  418  419  420\n",
      "  421  422  423  424  425  426  427  428  429  430  431  432  433  434\n",
      "  436  437  438  440  441  442  443  444  446  447  448  449  451  452\n",
      "  453  454  455  456  457  458  459  460  461  462  463  465  466  467\n",
      "  468  470  472  473  474  475  477  478  479  480  481  482  484  485\n",
      "  487  489  490  491  492  493  495  497  498  500  501  502  504  505\n",
      "  506  507  508  509  510  512  513  514  515  516  517  518  519  520\n",
      "  521  522  523  524  525  526  527  529  530  531  532  533  535  536\n",
      "  537  538  539  540  541  542  543  544  545  546  547  548  549  550\n",
      "  551  552  553  554  555  556  558  560  561  562  563  564  566  567\n",
      "  568  569  570  571  572  573  575  576  577  578  579  580  581  583\n",
      "  584  585  586  587  588  589  590  591  592  593  595  597  598  599\n",
      "  600  601  602  603  604  606  607  608  609  610  611  612  613  614\n",
      "  615  616  618  619  620  622  623  624  625  626  627  628  629  630\n",
      "  631  632  633  634  635  636  637  638  639  640  641  642  643  644\n",
      "  645  646  647  649  650  651  652  653  654  655  656  657  658  659\n",
      "  660  661  662  663  664  665  666  667  668  670  671  672  673  674\n",
      "  675  676  677  678  679  681  682  684  685  686  687  688  689  690\n",
      "  691  692  694  695  696  697  698  699  700  701  702  703  704  705\n",
      "  706  707  708  709  710  711  712  713  714  715  716  717  718  719\n",
      "  720  721  722  723  724  725  726  727  728  730  731  732  733  734\n",
      "  735  736  737  738  739  740  741  742  743  744  745  746  747  748\n",
      "  749  750  752  753  754  755  756  757  758  759  760  761  762  763\n",
      "  764  765  766  768  769  771  772  773  774  775  776  777  779  780\n",
      "  781  782  783  784  785  786  787  788  789  790  791  792  793  794\n",
      "  796  797  798  799  800  801  802  803  804  805  806  807  808  809\n",
      "  810  811  812  813  815  816  817  818  819  820  821  822  823  825\n",
      "  827  828  829  830  831  832  833  834  835  836  837  838  839  840\n",
      "  841  842  843  844  845  846  847  848  849  850  851  852  853  854\n",
      "  856  857  858  859  860  861  862  863  865  866  867  869  870  872\n",
      "  873  874  875  876  877  879  880  881  882  883  884  885  886  887\n",
      "  889  890  891  892  893  894  895  896  897  898  899  901  902  903\n",
      "  904  905  906  907  908  909  910  912  913  914  915  916  917  919\n",
      "  920  921  924  925  926  928  929  930  931  932  933  934  935  936\n",
      "  937  938  939  940  941  942  943  944  945  946  947  948  949  950\n",
      "  951  952  953  954  955  956  957  958  959  960  961  962  963  964\n",
      "  965  966  967  968  969  970  971  972  973  974  976  977  979  980\n",
      "  981  982  983  984  985  986  987  988  989  990  992  993  995  996\n",
      "  997  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1011 1012\n",
      " 1013 1014 1015 1016 1017 1019 1020 1021 1023 1024 1025 1026 1027 1028\n",
      " 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042\n",
      " 1043]\n",
      "(262, 300)\n",
      "(262, 1)\n",
      "(939, 300)\n",
      "(939, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/iforest.py:415: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "forest = IsolationForest()\n",
    "forest.fit(x_train)\n",
    "\n",
    "# Outlier indices for training\n",
    "outliers_training = forest.predict(x_train)\n",
    "outliers_training_indices = np.argwhere(outliers_training == 1).flatten()\n",
    "\n",
    "# Drop signal outliers in training data\n",
    "x_train = x_train[outliers_training == 1]\n",
    "y_train = y_train[outliers_training == 1]\n",
    "#x_train, y_train = drop_outliers_samples_isolation_forest(x_train, y_train)\n",
    "print(outliers_training_indices)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 300)\n",
      "(262, 1)\n",
      "(939, 300)\n",
      "(939, 1)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns, index=x_train.index)\n",
    "x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns, index=x_test.index)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 20, 'max_leaf_nodes': 250, 'min_samples_split': 3, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "rdf = fit_random_forest_with_grid_search(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.811374080214944"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "                      max_features='auto', max_leaf_nodes=250,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=3,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(max_depth= 20, max_leaf_nodes= 250, min_samples_split= 3, n_estimators= 1000) \n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8085104237567675"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.mean_squared_error(y_test , rf.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
